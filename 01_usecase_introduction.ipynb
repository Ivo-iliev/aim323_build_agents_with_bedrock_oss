{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873b300b-57df-416b-867b-e2a3120b2ed0",
   "metadata": {},
   "source": [
    "## Lab 1: AI Travel Assistant Use Case Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c436186-5d8a-4c94-824d-98cd353a73e9",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d0005-114e-4ad4-bcad-d52030415bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this first Lab, we will introduce the AI Travel Assistant use case, discuss the data sources, core features of AI Agents with Bedrock and Open Source Agent Framework LangChain and set up the packages and dependencies that is required for the rest of the Labs.\n",
    "\n",
    "The use case is to help create a bot which \n",
    "- Find `My Dream Destination` which finds similiar destinations based on the user preferences using `RAG`\n",
    "- Can `book travel` including hotel and flight\n",
    "- Leverages `Tools` to exeute the actions\n",
    "- Asks for `Human Confirmation` for critical exeution flows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ab8cc-75e4-4613-9d1a-125ff22e2780",
   "metadata": {},
   "source": [
    "### Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad3e7e-f713-45f1-bb28-06c3c254fece",
   "metadata": {},
   "source": [
    "Imagine having an AI assistant that can help you plan your next vacation, catering to all your preferences and needs. That's exactly what our \"AI Travel Assistant\" does—a cutting-edge, agentic AI system built using LangGraph and Amazon Bedrock Models. This assistant is designed to make travel planning seamless, offering personalized and data-driven suggestions every step of the way.\n",
    "\n",
    "We will cover following scenarios in this workshop\n",
    "\n",
    "1. **Lab 2: Introduction to the concepts of Agents including building blocks for LangGrah and ReACT agents. We will introduce memory concepts in this lab to show how to have 'Multi-Turn' conversations. We will show short term and long term memory:** \n",
    "\n",
    "Understand what it takes to build a agentic system.\n",
    "\n",
    "2. **Lab 3: Dream Destination Planning with Introduction of RAG to the pipeline for Customized Travel Itineraries. We will show different types of Agents incuding concepts like 'Human-In-The-Loop': We will also introduce Multi-Modal concept in this lab** \n",
    "\n",
    "Get personalized recommendations for your next dream destination. Create vacation plans tailored specifically to your preferences and based on your past travels.\n",
    "\n",
    "3. **Lab 4: Multi-Agent colloboration. We will leverage all the work from the previous labs and then assemble the building blocks to create a multi agent colloboration. We will use Similar Travelers Based Recommendations, book hotels and flights:** \n",
    "\n",
    "Receive recommendations influenced by the preferences of travelers in a similar age group and geographic location.Book, modify, and cancel flight and hotel reservations—all through one assistant.\n",
    "\n",
    "4. **Lab 5: [Optional] Dream Destination with crew.ai for comparisions:** \n",
    "\n",
    "Book, modify, and cancel flight and hotel reservations—all through one assistant.\n",
    "\n",
    "5. **Lab 6: Agentic Evaluation with RAGAS:** \n",
    "\n",
    "Understand how to evaluate the Agentic workflows using the open soyrce RAGAS library.\n",
    "\n",
    "\n",
    "The diagram will be similiar to this below\n",
    "\n",
    "<img src=\"./images/travel_agent_light.png\" width=\"65%\" alt='travel_agent_light.png'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ab837-5ca9-4658-8e97-f732f6b05c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Introduction to the data set we wil be using for this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad55ec-0299-4737-93fe-54767f697c46",
   "metadata": {},
   "source": [
    "#### 1. Travel History: \n",
    "\n",
    "We have synthetically created travel history dataset we will levearge in this workshop, it contains following fields\n",
    "\n",
    "- Name: The individual's full name.\n",
    "- Current_Location: The city where the individual currently resides.\n",
    "- Age: The individual's age in years.\n",
    "- Past_Travel_Destinations: Cities the individual has previously visited, listed as comma-separated values.\n",
    "- Number_of_Trips: The total count of trips the individual has taken.\n",
    "- Flight_Number: The flight number associated with each trip taken.\n",
    "- Departure_City: The city from which each flight departed.\n",
    "- Arrival_City: The destination city for each flight.\n",
    "- Flight_Date: The date of each flight taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718f278-3099-4b42-a81b-e6d632339cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/synthetic_travel_data.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575e3f0-d4d7-4508-a286-0e6a3d1ef713",
   "metadata": {},
   "source": [
    "#### 2. US and Europe City Dataset\n",
    "\n",
    "For the purpose of the Retrieval Augmented generation(RAG) demonstrations in the labs, we have synthetically created PDF datasets to be ingested in the Vector DBs. Following is a sample PDF organises in `/data/us` and `/data/europe` folders respectively. We have used an Agentic implementation to create the dataset. You can check this out in the optional notebook provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac8674-5c70-4d67-a8e8-39ff8cb5480e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./images/vegas.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7f421-9a23-462e-987c-10103b5fdfa1",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Model Access\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) offers a variety of foundation models (FMs) that are optimized for different use cases, making it a versatile platform for building generative AI applications. For agentic workflows, where models interact with tools, make decisions, and carry out complex tasks autonomously, choosing the right FM is crucial. For the purposes of this demostration, we will use the foundational models available on Amazon Bedrock with a focus on Anthropic Claude, Meta’s models, and Amazon’s own models, highlighting their capabilities for demonstration purposes. You can take a look [Supported foundation models in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175411a-c8dc-4610-9097-683ed2c03403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region = 'us-west-2'\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31adfd14-5ee4-403f-89ed-92d886f9c8e3",
   "metadata": {},
   "source": [
    "#### Anthropic's Claude in Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284454be-727e-4b04-81bf-01cedc2d9e0d",
   "metadata": {},
   "source": [
    "The `langchain_aws` package is a tool for integrating and interacting with Amazon Bedrock's language models through the LangChain framework. It provides following benefits\n",
    "\n",
    "- Simplified Access to Amazon Bedrock Models\n",
    "- Support for Conversational AI and Agentic Workflows\n",
    "\n",
    "- `from langchain_aws.chat_models.bedrock:` This imports the ChatBedrock class from the langchain_aws package, specifically from the chat_models module.\n",
    "- `ChatBedrock:` This is a class that allows interaction with Amazon Bedrock’s chat models, enabling users to create conversational AI applications using different foundational models available on Bedrock.\n",
    "\n",
    "Refer [Anthropic's Claude in Amazon Bedrock](https://aws.amazon.com/bedrock/claude/) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1ada",
   "metadata": {},
   "source": [
    "### Converse API\n",
    "\n",
    "<img src=\"./images/converse_api.png\" width=\"60%\" alt=\" conver api simplifies\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef8c2d-e7b8-4948-a76e-50c428e137ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --no-cache-dir  \\\n",
    "\"langchain==0.3.7\" \\\n",
    "\"langchain-aws==0.2.6\" \\\n",
    "\"langchain-community==0.3.5\" \\\n",
    "\"langchain-text-splitters==0.3.2\" \\\n",
    "\"langchainhub==0.1.20\" \\\n",
    "\"langgraph==0.2.45\" \\\n",
    "\"langgraph-checkpoint==2.0.2\" \\\n",
    "\"langgraph-sdk==0.1.35\" \\\n",
    "\"langsmith==0.1.140\" \\\n",
    "\"pypdf==3.8,<4\" \\\n",
    "\"ipywidgets>=7,<8\" \\\n",
    "\"matplotlib==3.9.0\" \\\n",
    "\"faiss-cpu==1.8.0\"\n",
    "\n",
    "%pip install -U --no-cache-dir boto3\n",
    "#%pip install grandalf==3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fc9c3-591f-48fb-a153-6ae0dfd6a4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "\n",
    "# Create a console object\n",
    "console = Console()\n",
    "\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock,\n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "response = llm.invoke(\"I am planning a trip  with my family to Hawai next summer, can you give me a travel iternary for 5 days\").content\n",
    "# Format the text as a rich Text object\n",
    "formatted_text = Text.from_markup(response)\n",
    "\n",
    "# Print the formatted response using rich\n",
    "console.print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb9e25-e947-49cf-b4e7-6c22d4ce91c7",
   "metadata": {},
   "source": [
    "### Course-grained model comparison\n",
    "\n",
    "In this section, we experiment with multiple models available on Amazon Bedrock and run course-grained evaluation on one of our task of interest. With the thousands of available models on the market, it is intractable to evaluation every single one. Hence, it is generally necessary to pre-filter for the ones that are not only from trusted providers, but have shown strong performance on a variety of benchmarks. \n",
    "\n",
    "Amazon Bedrock allows you to make a quick short-list by supporting a growing list providers such as Anthropic, Meta, Mistral, Cohere, AI21Labs, Stability AI and Amazon. This lets you start with a strong base to continue the model selection process.\n",
    "\n",
    "Next we perform course-grained model evalution on the following models to inform our initial choice of model for our task of interest:\n",
    "- Anthropic: Claude 3 Haiku\n",
    "\n",
    "For this workshop we have access to 1 model only. Ideally in your own accounts you can use multiple models to run this evaluation.\n",
    "\n",
    "To perform an initial evaluation, we create a small curated dataset of 10 examples. The optimal initial number of examples should be sufficiently big to roughly cover the types of queries our customers will send our model. Since this stage of the model evaluation process is meant to get a rough idea, the number of examples can be small.\n",
    "\n",
    "To start, our scenario can be described by summarization (**task**) of vacation destinations (**what**) asked by travelers (**who**) at the time of development (**when**) in English (**language**). The set of initial questions can be found in [examples.txt](./data/examples.txt). We could expand our test by changing one or more of the variables composing the scenario of interesting. For instance, we could generate equivalent examples, but asked by people who aren't travelers or by others speaking in any other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd4753-c613-415b-9e25-ce71aaeba980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/examples.txt\", \"r\") as file:\n",
    "    examples = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea783a12-03c3-4e37-a34b-1aa38400c7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Create a console object\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def generate_answers(\n",
    "    examples: list = [],\n",
    "    system_prompt: SystemMessage = None\n",
    "):\n",
    "    modelIds = [\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    ]\n",
    "    output = pd.DataFrame({\n",
    "        'example': [],\n",
    "        'Claude3Haiku': [],\n",
    "    })\n",
    "    for example in examples:\n",
    "        results = [example]\n",
    "        for modelId in modelIds:\n",
    "            messages = [\n",
    "                system_prompt if system_prompt else SystemMessage(content=\"\"),\n",
    "                HumanMessage(content=example)\n",
    "            ]\n",
    "            llm = ChatBedrock(\n",
    "                model_id=modelId,\n",
    "                client=bedrock,\n",
    "                beta_use_converse_api=True\n",
    "            )\n",
    "            resp = llm.invoke(messages).content\n",
    "            results.append(resp)\n",
    "        output.loc[len(output)] = results\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646b7a6-2c56-4451-a2f5-c71d0dd7d339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_example = examples\n",
    "output = generate_answers(one_example)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc4e7e-8b5e-40fd-90ce-44afb0cedff9",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we established \n",
    "- the use case scope of the labs covered in this workshop\n",
    "- datasets we will be using all the labs\n",
    "- Amazon bedrock models used in this\n",
    "\n",
    "Please proceed to the next labs"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
